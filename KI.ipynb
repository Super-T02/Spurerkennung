{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Entwicklungsumgebungen\\Anaconda\\envs\\Bildverarbeitung\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import scipy.special\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from model.model import parsingNet\n",
    "\n",
    "lane_colors = [(0,0,255),(0,255,0),(255,0,0)]\n",
    "\n",
    "tusimple_row_anchor = [ 64,  68,  72,  76,  80,  84,  88,  92,  96, 100, 104, 108, 112,\n",
    "\t\t\t116, 120, 124, 128, 132, 136, 140, 144, 148, 152, 156, 160, 164,\n",
    "\t\t\t168, 172, 176, 180, 184, 188, 192, 196, 200, 204, 208, 212, 216,\n",
    "\t\t\t220, 224, 228, 232, 236, 240, 244, 248, 252, 256, 260, 264, 268,\n",
    "\t\t\t272, 276, 280, 284]\n",
    "\n",
    "model_path = \"model/tusimple_18.pth\"\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig():\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.img_w = 1280\n",
    "\t\tself.img_h = 720\n",
    "\t\tself.row_anchor = tusimple_row_anchor\n",
    "\t\tself.griding_num = 100\n",
    "\t\tself.cls_num_per_lane = 56\n",
    "\n",
    "class LaneDetectiion():\n",
    "    def __init__(self, model_path, use_gpu=False):\n",
    "\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        # Load model configuration based on the model type\n",
    "        self.cfg = ModelConfig()\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = self.initialize_model(model_path, self.cfg, use_gpu)\n",
    "\n",
    "        # Initialize image transformation\n",
    "        self.img_transform = self.initialize_image_transform()\n",
    "    \n",
    "    def initialize_model(self,model_path, cfg, use_gpu):\n",
    "        # Load the model architecture\n",
    "        net = parsingNet(pretrained = False, backbone='18', cls_dim = (cfg.griding_num+1,cfg.cls_num_per_lane,4))\n",
    "\n",
    "\n",
    "        # Load the weights from the downloaded model\n",
    "        if use_gpu:\n",
    "            net = net.cuda()\n",
    "            state_dict = torch.load(model_path, map_location='cuda')['model'] # CUDA\n",
    "        else:\n",
    "            state_dict = torch.load(model_path, map_location='cpu')['model'] # CPU\n",
    "\n",
    "        compatible_state_dict = {}\n",
    "        for k, v in state_dict.items():\n",
    "            if 'module.' in k:\n",
    "                compatible_state_dict[k[7:]] = v\n",
    "            else:\n",
    "                compatible_state_dict[k] = v\n",
    "\n",
    "        # Load the weights into the model\n",
    "        net.load_state_dict(compatible_state_dict, strict=False)\n",
    "        net.eval()\n",
    "\n",
    "        return net\n",
    "\n",
    "    def initialize_image_transform(self):\n",
    "\t\t# Create transfom operation to resize and normalize the input images\n",
    "        img_transforms = transforms.Compose([\n",
    "\t\t\ttransforms.Resize((288, 800)),\n",
    "\t\t\ttransforms.ToTensor(),\n",
    "\t\t\ttransforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "\t\t])\n",
    "\n",
    "        return img_transforms\n",
    "\n",
    "    def detect_lanes(self, image, draw_points=True):\n",
    "\n",
    "        input_tensor = self.prepare_input(image)\n",
    "\n",
    "        # Perform inference on the image\n",
    "        output = self.inference(input_tensor)\n",
    "\n",
    "        # Process output data\n",
    "        self.lanes_points, self.lanes_detected = self.process_output(output, self.cfg)\n",
    "\n",
    "\n",
    "        # Draw depth image\n",
    "        visualization_img = self.draw_lanes(image, self.lanes_points, self.lanes_detected, self.cfg, draw_points)\n",
    "\n",
    "        return visualization_img\n",
    "\n",
    "    def prepare_input(self, img):\n",
    "        # Transform the image for inference\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_pil = Image.fromarray(img)\n",
    "        input_img = self.img_transform(img_pil)\n",
    "        input_tensor = input_img[None, ...]\n",
    "\n",
    "        if self.use_gpu:\n",
    "            if not torch.backends.mps.is_built():\n",
    "                input_tensor = input_tensor.cuda()\n",
    "\n",
    "        return input_tensor\n",
    "\n",
    "    def inference(self, input_tensor):\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def process_output(self,output, cfg):\t\t\n",
    "        # Parse the output of the model\n",
    "        processed_output = output[0].data.cpu().numpy()\n",
    "        processed_output = processed_output[:, ::-1, :]\n",
    "        prob = scipy.special.softmax(processed_output[:-1, :, :], axis=0)\n",
    "        idx = np.arange(cfg.griding_num) + 1\n",
    "        idx = idx.reshape(-1, 1, 1)\n",
    "        loc = np.sum(prob * idx, axis=0)\n",
    "        processed_output = np.argmax(processed_output, axis=0)\n",
    "        loc[processed_output == cfg.griding_num] = 0\n",
    "        processed_output = loc\n",
    "\n",
    "\n",
    "        col_sample = np.linspace(0, 800 - 1, cfg.griding_num)\n",
    "        col_sample_w = col_sample[1] - col_sample[0]\n",
    "\n",
    "        lanes_points = []\n",
    "        lanes_detected = []\n",
    "\n",
    "        max_lanes = processed_output.shape[1]\n",
    "        for lane_num in range(max_lanes):\n",
    "            lane_points = []\n",
    "            # Check if there are any points detected in the lane\n",
    "            if np.sum(processed_output[:, lane_num] != 0) > 2:\n",
    "\n",
    "                lanes_detected.append(True)\n",
    "\n",
    "                # Process each of the points for each lane\n",
    "                for point_num in range(processed_output.shape[0]):\n",
    "                    if processed_output[point_num, lane_num] > 0:\n",
    "                        lane_point = [int(processed_output[point_num, lane_num] * col_sample_w * cfg.img_w / 800) - 1, int(cfg.img_h * (cfg.row_anchor[cfg.cls_num_per_lane-1-point_num]/288)) - 1 ]\n",
    "                        lane_points.append(lane_point)\n",
    "            else:\n",
    "                lanes_detected.append(False)\n",
    "\n",
    "            lanes_points.append(lane_points)\n",
    "        return np.array(lanes_points), np.array(lanes_detected)\n",
    "        \n",
    "    def draw_lanes(self,input_img, lanes_points, lanes_detected, cfg, draw_points=True):\n",
    "        # Write the detected line points in the image\n",
    "        visualization_img = cv2.resize(input_img, (cfg.img_w, cfg.img_h), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        # Draw a mask for the current lane\n",
    "        if(lanes_detected[1] and lanes_detected[2]):\n",
    "            lane_segment_img = visualization_img.copy()\n",
    "            \n",
    "            cv2.fillPoly(lane_segment_img, pts = [np.vstack((lanes_points[1],np.flipud(lanes_points[2])))], color =(255,191,0))\n",
    "            visualization_img = cv2.addWeighted(visualization_img, 0.7, lane_segment_img, 0.3, 0)\n",
    "\n",
    "        if(draw_points):\n",
    "            for lane_num,lane_points in enumerate(lanes_points):\n",
    "                if lane_num > 2:\n",
    "                    break\n",
    "                for lane_point in lane_points:\n",
    "                    cv2.circle(visualization_img, (lane_point[0],lane_point[1]), 3, lane_colors[lane_num], -1)\n",
    "\n",
    "        return visualization_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Entwicklungsumgebungen\\Anaconda\\envs\\Bildverarbeitung\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Entwicklungsumgebungen\\Anaconda\\envs\\Bildverarbeitung\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\benny\\AppData\\Local\\Temp\\ipykernel_36636\\3288233839.py:131: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(lanes_points), np.array(lanes_detected)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \t\u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mif\u001b[39;00m ret:\t\n\u001b[1;32m---> 18\u001b[0m \toutput_img \u001b[39m=\u001b[39m lane_detector\u001b[39m.\u001b[39;49mdetect_lanes(frame)\n\u001b[0;32m     20\u001b[0m \tcv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mDetected lanes\u001b[39m\u001b[39m\"\u001b[39m, output_img)\n\u001b[0;32m     22\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn [3], line 71\u001b[0m, in \u001b[0;36mLaneDetectiion.detect_lanes\u001b[1;34m(self, image, draw_points)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlanes_points, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlanes_detected \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_output(output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[0;32m     70\u001b[0m \u001b[39m# Draw depth image\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m visualization_img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdraw_lanes(image, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlanes_points, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlanes_detected, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfg, draw_points)\n\u001b[0;32m     73\u001b[0m \u001b[39mreturn\u001b[39;00m visualization_img\n",
      "Cell \u001b[1;32mIn [3], line 149\u001b[0m, in \u001b[0;36mLaneDetectiion.draw_lanes\u001b[1;34m(self, input_img, lanes_points, lanes_detected, cfg, draw_points)\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    148\u001b[0m         \u001b[39mfor\u001b[39;00m lane_point \u001b[39min\u001b[39;00m lane_points:\n\u001b[1;32m--> 149\u001b[0m             cv2\u001b[39m.\u001b[39mcircle(visualization_img, (lane_point[\u001b[39m0\u001b[39m],lane_point[\u001b[39m1\u001b[39m]), \u001b[39m3\u001b[39m, lane_colors[lane_num], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    151\u001b[0m \u001b[39mreturn\u001b[39;00m visualization_img\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"img/Udacity/project_video.mp4\")\n",
    "\n",
    "# Initialize lane detection model\n",
    "lane_detector = LaneDetectiion(model_path, use_gpu)\n",
    "\n",
    "cv2.namedWindow(\"Detected lanes\", cv2.WINDOW_NORMAL)\n",
    "i = 1\n",
    "ret, frame = cap.read()\n",
    "while ret:\n",
    "\ttry:\n",
    "\t\t# Read frame from the video\n",
    "\t\tret, frame = cap.read()\n",
    "\t\t\n",
    "\texcept:\n",
    "\t\tcontinue\n",
    "\n",
    "\tif ret:\t\n",
    "\t\toutput_img = lane_detector.detect_lanes(frame)\n",
    "\n",
    "\t\tcv2.imshow(\"Detected lanes\", output_img)\n",
    "\n",
    "\telse:\n",
    "\t\tbreak\n",
    "\n",
    "\t# Press key q to stop\n",
    "\tif cv2.waitKey(1) == ord('q'):\n",
    "\t\tbreak\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Bildverarbeitung')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0a3075694810a7bb993da77f9c64a653d81b0695ead550ad875383aa24f0556"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
