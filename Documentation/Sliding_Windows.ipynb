{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erklärung SlidingWindows\n",
    "\n",
    "Dieser Abschnitt erklärt die meisten Methoden des Objektes SlidingWindows. Weggelassen wurden Methoden, welche nur für die Entwicklung benötigt wurden und im Produktivcode nicht aufgerufen werden (Debugging Methoden).\n",
    "### Disclaimer\n",
    "Der Code ist nicht lauffähig, dafür existieren die Python Skripte. Das Notebook stellt lediglich eine Dokumentation dar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbesserungen, welche an SlidingWindows vorgenommen wurden\n",
    "\n",
    "- Wenn eine Spur erkannt wurde, wird der Punkt, an welchem sie beginnt gespeichert. Dieser Punkt wird dann im nächsten Frame als Ausgangspunkt für die Spurensuche genommen. Dadurch wird nicht mehr eine ganze Bildhälfte durchsucht, sondern nur das unmittelbare Umfeld des im letzten Frame gefundenen Punktes. Dies spart Rechenkapazität ein und sorgt für genauere und schnellere Ergebnisse. Dies ist genauer beschrieben in set_context.\n",
    "- Die Windows werden nach oben hin immer breiter. Dadurch kann bei krummen Verlaufen die Spur weiter in der Distanz erkannt werden. Die Windows alle breit zu machen zieht dabei unnötige Rechenkapazität. Beschrieben ist dies genauer in generate_window\n",
    "\n",
    "<img src=\"Images/Sliding_windows_bad.png\" width=\"500\">\n",
    "\n",
    "<img src=\"Images/Sliding_windows_really_good.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Dies sind die spezifischen Imports für SlidingWindows. \n",
    "Wichtig sind hier die perspective transform als per und preprocess als pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "import calib as cal\n",
    "import perspective_transform as per\n",
    "import preprocess as pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class SlidingWindow mit init Funktion\n",
    "\n",
    "Es werden benötigte Objekte (wie Transformation und Preprocess) initialisiert. die last_draw_info und last_frames werden für die unten aufgeführte Spurverfolgung benötigt. Es wird also nicht in jedem Frame die Spur neu gesucht, sondern versucht, die vorher bereits erkannte Spur zu verfolgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindow():\n",
    "    \n",
    "    def __init__(self, debug = False, debug_plots = False) -> None:\n",
    "        \"\"\"Constructor for the SlidingWindow class\n",
    "\n",
    "        Args:\n",
    "            debug (bool, optional): Debug mode. Defaults to False.\n",
    "            debug_plots (bool, optional): Debug mode with plot of histogram. Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.transformation = per.Transformation(debug)\n",
    "        self.pre = pre.Preprocess()\n",
    "        self.debug = debug\n",
    "        self.debug_plots = debug_plots\n",
    "        self.loaded = False   \n",
    "        \n",
    "        # Remember last frames and draw information\n",
    "        self.last_draw_info = None\n",
    "        self.last_frame_right_x = None\n",
    "        self.last_frame_left_x = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load config\n",
    "Hier werden spezifische Konfigurationen für Sliding Window aus der Konfigurationsdatei geladen und auf Vollständigkeit überprüft. Es gibt zwei verschiedene Konfigurationen, eine für das default Video, eine für das Fortgeschrittene.\n",
    "Wenn alle Felder gefunden wurden, werden sie als Attribute dem Objekt zugewiesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(self, path):\n",
    "    if not os.path.exists(path):\n",
    "        return print('File '+ path +' not found')\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    if not config:\n",
    "        return 'Error: Config not found'\n",
    "    if not 'SLIDING_WINDOWS' in config.keys():\n",
    "        return 'Error: SLIDING_WINDOWS is missing'\n",
    "    if not 'N_WINDOWS' in config['SLIDING_WINDOWS'].keys():\n",
    "        return 'Error: N_WINDOWS is missing'\n",
    "    if not 'MARGIN' in config['SLIDING_WINDOWS'].keys():\n",
    "        return 'Error: MARGIN is missing'\n",
    "    if not 'MIN_PIX' in config['SLIDING_WINDOWS'].keys():\n",
    "        return 'Error: MIN_PIX is missing'\n",
    "    if not 'THRESH' in config['SLIDING_WINDOWS'].keys():\n",
    "        return 'Error: THRESH is missing'\n",
    "    if not 'LANE_WIDTH_FOR_SEARCH' in config['SLIDING_WINDOWS'].keys():\n",
    "        return 'Error: LANE_WIDTH_FOR_SEARCH is missing'\n",
    "    if not 'SCALING_OF_BOX_WIDTH' in config['SLIDING_WINDOWS'].keys():\n",
    "        return 'Error: SCALING_OF_BOX_WIDTH is missing'\n",
    "    if not 'MIN_COLOR' in config['SLIDING_WINDOWS'].keys():\n",
    "        return 'Error: MIN_COLOR is missing'\n",
    "    if not 'MAX_COLOR' in config['SLIDING_WINDOWS'].keys():\n",
    "        return 'Error: MAX_COLOR is missing'\n",
    "    if not 'TRANS_MATRIX' in config['SLIDING_WINDOWS'].keys():\n",
    "        return 'Error: TRANS_MATRIX is missing'\n",
    "    if not 'HIT_X_LEFT' in config['SLIDING_WINDOWS'].keys():\n",
    "        return 'Error: HIT_X_LEFT is missing'\n",
    "    if not 'HIT_Y_LEFT' in config['SLIDING_WINDOWS'].keys():\n",
    "        return 'Error: HIT_Y_LEFT is missing'\n",
    "    if not 'HIT_X_RIGHT' in config['SLIDING_WINDOWS'].keys():\n",
    "        return 'Error: HIT_X_RIGHT is missing'\n",
    "    if not 'HIT_Y_RIGHT' in config['SLIDING_WINDOWS'].keys():\n",
    "        return 'Error: HIT_Y_RIGHT is missing'\n",
    "    \n",
    "    self.n_windows = config['SLIDING_WINDOWS']['N_WINDOWS']\n",
    "    self.margin = config['SLIDING_WINDOWS']['MARGIN']\n",
    "    self.min_pix = config['SLIDING_WINDOWS']['MIN_PIX']\n",
    "    self.thresh = config['SLIDING_WINDOWS']['THRESH']\n",
    "    self.lane_width_for_search = config['SLIDING_WINDOWS']['LANE_WIDTH_FOR_SEARCH']\n",
    "    self.scaling_of_box_width = config['SLIDING_WINDOWS']['SCALING_OF_BOX_WIDTH']\n",
    "    self._min_color = config['SLIDING_WINDOWS']['MIN_COLOR']\n",
    "    self._max_color = config['SLIDING_WINDOWS']['MAX_COLOR']\n",
    "    self.trans_matrix = config['SLIDING_WINDOWS']['TRANS_MATRIX']\n",
    "    self._hit_x_left = config['SLIDING_WINDOWS']['HIT_X_LEFT']\n",
    "    self._hit_y_left = config['SLIDING_WINDOWS']['HIT_Y_LEFT']\n",
    "    self._hit_x_right = config['SLIDING_WINDOWS']['HIT_X_RIGHT']\n",
    "    self._hit_y_right = config['SLIDING_WINDOWS']['HIT_Y_RIGHT']\n",
    "    \n",
    "    self.loaded = True\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute\n",
    "Diese Methode stellt die 'Main' der Klasse dar. Um SlidingWindows von außerhalb zu starten, muss diese Methode ausgeführt werden. Sie ruft dann die jeweiligen Hilfsfunktionen auf. \n",
    "Der grobe Ablauf ist:\n",
    "- Das Bild vorverarbeiten (Mit versch. Filtern und Transformation in Vogelperspektive)\n",
    "- Es werden Suchbereiche für die Fahrspuren mit dem Histogram festgelegt\n",
    "- Die einzelnen Windows werden erstellt und angewendet\n",
    "- Die Koordinaten für die Fahrspur werden berechnet\n",
    "- Wenn das Ergebnis plausibel ist, dann einzeichnen der Fahrspur in das ursprüngliche Bild\n",
    "\n",
    "Anschließend wird das Bild zurückgegeben. Falls das Ergebnis nicht plausibel ist, wird das ursprüngliche Bild zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(self, img):\n",
    "    \"\"\"Execute the sliding window algorithm\n",
    "\n",
    "    Args:\n",
    "        img (Image): Current frame\n",
    "\n",
    "    Returns:\n",
    "        Image: Processed frame\n",
    "    \"\"\"\n",
    "    if not self.loaded:\n",
    "        return False\n",
    "    \n",
    "    # Preprocess the image\n",
    "    img_transformed, M_reverse = self._preprocess(img)\n",
    "\n",
    "    if self.debug:\n",
    "        cv.imshow('transformed', img_transformed)\n",
    "        \n",
    "            \n",
    "    # Set local vars\n",
    "    hist = self.get_histogram(img_transformed)\n",
    "    img_y_shape = img_transformed.shape[0]\n",
    "    img_x_shape = img_transformed.shape[1]\n",
    "    self.set_context(img_transformed, hist)\n",
    "\n",
    "    # Generate the windows\n",
    "    for i_window in range(self.n_windows):\n",
    "        self._generate_window(img_y_shape, i_window)\n",
    "\n",
    "    # Get the drawing information\n",
    "    draw_info = self._generate_line_coordinates(img_y_shape, img_x_shape)\n",
    "\n",
    "    if not draw_info:\n",
    "\n",
    "        if not self.last_draw_info:\n",
    "            return img\n",
    "        else:\n",
    "            draw_info = self.last_draw_info\n",
    "    \n",
    "    else:\n",
    "        self.last_draw_info = draw_info\n",
    "\n",
    "    # Draw the line\n",
    "    if self.check_plausibility(draw_info, img.shape): img = self._draw_lane_area(img, img_transformed, M_reverse, draw_info)\n",
    "\n",
    "    # Return finished frame\n",
    "    return img     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check plausibility\n",
    "\n",
    "Überprüft die erhaltenen Spuren. Wenn sich eine Koordinate der Spur im einer der beiden unteren Ecken befindet oder sich die Polynome (im Sichtbereich) schneiden, wird das Ergebnis als nicht gültig angesehen. Die Größe der Boxen kommt auf das Video an, also wie nahe das Fahrzeug an welcher Spur fährt. \n",
    "Wenn sich die Spuren kreuzen weicht mindestens eine von der reellen Spur ab (Fahrspuren kreuzen sich in den allermeisten Fällen nicht). Außerdem ist beim geradeaus fahren innerhalb einer Spur auf einem Highway keine Spur in den Ecken zu erwarten, was das Ergebnis also auch unplausibel macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_plausibility(self, draw_info, img_shape) -> bool:\n",
    "    y_values = draw_info['PLOT_Y']\n",
    "    left_fit_x = draw_info['LEFT_FIT_X']\n",
    "    right_fit_x = draw_info['RIGHT_FIT_X']\n",
    "    \n",
    "    # Check for the hit boxes\n",
    "    if any(left_fit_x[self._hit_y_left:] <= self._hit_x_left):\n",
    "        return False\n",
    "    \n",
    "    if any(right_fit_x[self._hit_y_right:] >= img_shape[1] + self._hit_x_right):\n",
    "        return False\n",
    "    \n",
    "    # Check for crossing lines\n",
    "    if any(left_fit_x >= right_fit_x):\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "Zuerst wird jeder Gelb-Wert auf weiß gemappt. Dies wird gemacht, um nachher den Threshold besser einstellen zu können. Gelb ist in der Grau-Darstellung dunkler als weiß, dadurch konnte der Threshold nicht präzise genug eingestellt werden und es wurden viele Störsignale erkannt. Mithilfe dieser Transformation der gelben Farbwerte konnte die Threshold Vorverarbeitung deutlich verbessert werden, was zu einer deutlichen Präzisionssteigerung der Erkennung geführt hat.\n",
    "\n",
    "Anschließend wird das Bild in Graustufe konvertiert und ein Gauss Filter angewandt. Das Bild wird dann in Vogelperspektive transformiert und anschließend wird noch der Threshold darauf angewandt. Es werden das vorverarbeitete Bild und die Matrix zur Rückumwandlung zurückgegeben. Letztere wird später zur Darstellung der erkannten Spuren verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(self, img):\n",
    "    \"\"\"Preprocess the image, apply filters and transformations\n",
    "\n",
    "    Args:\n",
    "        img (Image): Current Frame\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Transformed image and the reversed transformation matrix\n",
    "    \"\"\"\n",
    "    # Find the yellow line\n",
    "    if self._min_color and self._max_color:\n",
    "        img = self.pre.map_color(img, self._min_color, self._max_color)\n",
    "        if self.debug:\n",
    "            cv.imshow('yellow', img)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    img = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Apply gaussian blur\n",
    "    img = self.pre.gauss(img)\n",
    "    \n",
    "    # Threshold the image to areal view\n",
    "    img_transformed, M_reversed = self.transformation.transform_image_perspective(img, self.trans_matrix)\n",
    "    \n",
    "    # Apply threshold\n",
    "    img_transformed = self.pre.threshold(img_transformed, self.thresh)\n",
    "\n",
    "    return img_transformed, M_reversed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get histogram\n",
    "Generiert das Histogram der unteren Hälfte eines Bildes. Dies hat den Zweck, Störungen im oberen Bereich nicht zu beachten. Enthält im Produktivcode noch Debugging Möglichkeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram(self, img):\n",
    "    \"\"\"Generate the histogram of the transformed image\n",
    "\n",
    "    Args:\n",
    "        img (Image): Current Frame\n",
    "\n",
    "    Returns:\n",
    "        List: Histogram of the image\n",
    "    \"\"\"\n",
    "    # Get the histogram of the image\n",
    "    histogram = np.sum(img[img.shape[0]//2:,:], axis=0)\n",
    "\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set context\n",
    "In dieser Methode wird der Bereich des Bildes für die eigentliche Erkennung der Spur definiert. Dies ist vor allem davon abhängig, ob im letzten Frame eine Spur erkannt wurde, welche nun verfolgt werden kann, oder ob die Spur neu gesucht werden muss. Es wird dabei zwischen linker und rechter Spur unterschieden, da es möglich ist, nur eine der beiden Spuren zu erkennen / zu speichern. Wenn keine Spur aus dem letzten Frame gespeichert wurde, wird das Bild mittig geteilt und der Suchbereich ist die komplette Hälfte. Wenn die Position der Spur aus dem letzten Frame gespeichert wurde, wird der Bereich um diesen X Wert ausgewählt. Das Offset für diese Grenzen um den X Wert wird in der Konfiguration gespeichert und in lane_width_for_search gespeichert. Default-mäßig liegt dieser Wert bei 10 Pixeln. Damit wird der Suchbereich der Spur von Halbes Bild (ca 700 Pixel) auf das 10 Pixel Umfeld der zuletzt erkannten Spur eingeschränkt (also 20 Pixel). Mit dieser Methode wird Rechenkapazität eingespart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_context(self, img, hist):\n",
    "    \"\"\"Generate the search context for the windows (based on the previous frame)\n",
    "\n",
    "    Args:\n",
    "        img (Image): Current Frame\n",
    "        hist (List): Histogram of the current frame\n",
    "    \"\"\"\n",
    "    # Generate black layered image\n",
    "    mid = img.shape[1]//2\n",
    "\n",
    "    if not self.last_frame_left_x:\n",
    "        # Divide the histogram into two parts\n",
    "        leftx_base = np.argmax(hist[:mid])\n",
    "    else:\n",
    "        left_negative = self.last_frame_left_x - self.lane_width_for_search\n",
    "        left_positive = self.last_frame_left_x + self.lane_width_for_search\n",
    "        if left_negative < 1:\n",
    "            left_negative = 1\n",
    "            left_positive = mid\n",
    "        leftx_base = np.argmax(hist[left_negative : left_positive]) + left_negative\n",
    "        self.last_frame_left_x = None\n",
    "        \n",
    "    if not self.last_frame_right_x:\n",
    "        # Divide the histogram into to parts\n",
    "        rightx_base = np.argmax(hist[mid:]) + mid\n",
    "    else:\n",
    "        right_negative = self.last_frame_right_x - self.lane_width_for_search\n",
    "        right_positive = self.last_frame_right_x + self.lane_width_for_search\n",
    "        if right_positive > img.shape[1] - 1:\n",
    "            right_negative = mid\n",
    "            right_positive = img.shape[1] - 1 \n",
    "        rightx_base = np.argmax(hist[right_negative : right_positive]) + right_negative\n",
    "        self.last_frame_right_x = None\n",
    "\n",
    "\n",
    "    # Number of sliding windows in the frame\n",
    "    # self.n_windows = 10\n",
    "    self.window_height = img.shape[0]//self.n_windows\n",
    "\n",
    "    # Find coordinates which are not zero\n",
    "    nonzero = img.nonzero()\n",
    "    self.nonzeroy = np.array(nonzero[0])\n",
    "    self.nonzerox = np.array(nonzero[1])\n",
    "\n",
    "    self.current_leftx = leftx_base\n",
    "    self.current_rightx = rightx_base\n",
    "\n",
    "    self.left_lane_inds = []\n",
    "    self.right_lane_inds = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate window\n",
    "Diese Funktion wird in execute in einer Schleife aufgerufen, für jede window Position einzeln. Die Methode generiert also für den angegebenen Suchbereich ein Window und positioniert es an der Stelle, wo sie etwas erkannt hat. Sie speichert die Position des untersten Frames (also die Position der Spurmarkierung am nächsten beim Auto). Die Indices werden in den lane_inds Attributen des Objektes gespeichert. Wichtig zu beachten ist hier die Vergrößerung der Windows. Diese werden nach obenhin (also in die Ferne) immer breiter, da der in Vogelperspektive transformierte Kurvenverlauf sich oben deutlich stärker krümmen kann als unten. Ohne diese Vergrößerung konnten Linien nicht so sehr in die Weite erkannt werden, da sie sich oft zu stark krümmten. Die Fenster alle so groß zu machen wie benötigt ist allerdings rechenintensiver. Also wurde diese Fensterbreite linear zunehmend designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_window(self, img_y_shape, index):\n",
    "    \"\"\"Generate the window for the current index\n",
    "\n",
    "    Args:\n",
    "        img_y_shape (int): y shape of the image\n",
    "        index (int): index of the current window\n",
    "    \"\"\"\n",
    "    # Set current window coordinates\n",
    "    win_y_low = img_y_shape - (index + 1) * self.window_height\n",
    "    win_y_high = img_y_shape - index * self.window_height\n",
    "    \n",
    "    # Define box-window coordinates\n",
    "    win_xleft_low = self.current_leftx - (self.margin + index * self.scaling_of_box_width)\n",
    "    win_xleft_high = self.current_leftx + (self.margin + index * self.scaling_of_box_width)\n",
    "    win_xright_low = self.current_rightx - (self.margin + index * self.scaling_of_box_width)\n",
    "    win_xright_high = self.current_rightx + (self.margin + index * self.scaling_of_box_width)\n",
    "\n",
    "    # Get the indices where the coordinates of the image are not\n",
    "    # zero but in the window (defined by the win_y_low, ...)\n",
    "    left_inds = ((self.nonzeroy >= win_y_low) & (self.nonzeroy < win_y_high) & (self.nonzerox >= win_xleft_low) & (self.nonzerox < win_xleft_high)).nonzero()[0]\n",
    "    right_inds = ((self.nonzeroy >= win_y_low) & (self.nonzeroy < win_y_high) & (self.nonzerox >= win_xright_low) & (self.nonzerox < win_xright_high)).nonzero()[0]\n",
    "    self.left_lane_inds.append(left_inds)\n",
    "    self.right_lane_inds.append(right_inds)\n",
    "\n",
    "    # Change the current indices\n",
    "    if len(left_inds) > self.min_pix:\n",
    "        self.current_leftx = int(np.mean(self.nonzerox[left_inds]))\n",
    "        if index == 0:\n",
    "            self.last_frame_left_x = self.current_leftx\n",
    "    if len(right_inds) > self.min_pix:\n",
    "        self.current_rightx = int(np.mean(self.nonzerox[right_inds]))\n",
    "        if index == 0:\n",
    "            self.last_frame_right_x = self.current_rightx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate line coordinates\n",
    "Diese Methode generiert aus den erkannten Punkten der generate_windows Methode vollständige Polynome für die Spuren. Wenn Punkte gefunden wurden, werden diese per polyfit() Funktion umgewandelt. Es wird ein dictionary mit allen wichtigen Werten und Funktionsparametern zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_line_coordinates(self, img_y_shape, img_x_shape):\n",
    "    \"\"\"Generate the line coordinates for the left and right lane\n",
    "\n",
    "    Args:\n",
    "        img_y_shape (int): shape of the image in y direction\n",
    "        img_x_shape (int): shape of the image in x direction\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary with the left and right lane coordinates\n",
    "    \"\"\"\n",
    "    # Flatten array\n",
    "    self.left_lane_inds = np.concatenate(self.left_lane_inds)\n",
    "    self.right_lane_inds = np.concatenate(self.right_lane_inds)\n",
    "\n",
    "    leftx = self.nonzerox[self.left_lane_inds]\n",
    "    lefty = self.nonzeroy[self.left_lane_inds]\n",
    "    rightx = self.nonzerox[self.right_lane_inds]\n",
    "    righty = self.nonzeroy[self.right_lane_inds]\n",
    "\n",
    "    # Check wether lines are detected:\n",
    "    if len(leftx) <= 0 or len(lefty) <= 0 or len(rightx) <= 0 or len(righty) <= 0:\n",
    "        # Prepare return values\n",
    "        ret = {}\n",
    "        return ret\n",
    "\n",
    "    # Create line\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    # Show plot of the created lines\n",
    "    plot_y = np.linspace(0, img_y_shape - 1, img_y_shape)\n",
    "    left_fit_x = left_fit[0] * plot_y**2 + left_fit[1] * plot_y + left_fit[2]\n",
    "    right_fit_x = right_fit[0] * plot_y**2 + right_fit[1] * plot_y + right_fit[2]\n",
    "\n",
    "    # Prepare return values\n",
    "    return {\n",
    "        'LEFT_X': leftx,\n",
    "        'RIGHT_X': rightx,\n",
    "        'LEFT_FIT_X': left_fit_x,\n",
    "        'RIGHT_FIT_X': right_fit_x,\n",
    "        'PLOT_Y': plot_y\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw lane area\n",
    "Zeichnet die gefundene Fahrspur in das ursprüngliche Video ein. \n",
    "Zeichnet hierfür in neues Null-Array (Form von Vogelperspektive-Bild) die Fläche ein und transformiert dann dieses neue Bild zurück in die normale Perspektive. Anschließend wird das neue Bild mit den Markierungen über das eigentliche Image gelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_lane_area(self, original_img, transformed_img, M_reversed, draw_info):\n",
    "    \"\"\"Draw the lane area on the original image\n",
    "\n",
    "    Args:\n",
    "        original_img (Image): original image\n",
    "        transformed_img (Image): transformed image\n",
    "        M_reversed (List): reversed transformation matrix\n",
    "        draw_info (Dict): dictionary with the coordinates of the lane\n",
    "\n",
    "    Returns:\n",
    "        Image: image with the lane area\n",
    "    \"\"\"\n",
    "    # Unpack draw Info\n",
    "    left_fit_x = draw_info['LEFT_FIT_X']\n",
    "    right_fit_x = draw_info['RIGHT_FIT_X']\n",
    "    plot_y = draw_info['PLOT_Y']\n",
    "\n",
    "    transformed_zero = np.zeros_like(transformed_img).astype(np.uint8)\n",
    "    color_transformed = np.dstack((transformed_zero, transformed_zero, transformed_zero))\n",
    "\n",
    "    # Generate the points of the lane\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fit_x, plot_y]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fit_x, plot_y])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the driving lane in the transformed image\n",
    "    cv.fillPoly(color_transformed, np.int_([pts]), (0,255,0))\n",
    "\n",
    "    # Untransform the image back \n",
    "    new_transformed = cv.warpPerspective(color_transformed, M_reversed, (original_img.shape[1], original_img.shape[0]))\n",
    "    result = cv.addWeighted(original_img, 1, new_transformed, 0.3, 0)\n",
    "\n",
    "    # Show the transformed aria\n",
    "    if self.debug: cv.imshow(\"Transformed Aria\", new_transformed)\n",
    "\n",
    "    return result   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Bildverarbeitung')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73e9040337f818fd60409dcdacdc763e91952f89d50108fc628d40f1d2b9a0fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
