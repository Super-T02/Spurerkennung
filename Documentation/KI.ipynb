{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping\n",
    "Diese Datei enthält das initiale Prototyping bevor auf normale python Skripte umgestiegen wurde. Diese Datei dient für\n",
    "die Nachvollziehbarkeit und Transparenz, wie die Finale Lösung entwickelt wurde.\n",
    "\n",
    "## Disclaimer\n",
    "Diese Datei enthält nicht die Finale Lösung. Die Lösung ist in den Python Skripten im `/src` Verzeichnis. Zudem können\n",
    "Konfigurationen, Methoden und Beschreibungen in dieser Datei Abweichen. Sie enthält also nur das initiale Prototyping\n",
    "und die daraus gewonnenen Erkenntnisse."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialer Import benutzter Bibliotheken\n",
    "In diesem Abschnitt werden die benötigten Bibliotheken und globale Variablen geladen. \n",
    "\n",
    "Der KI-Ansatz benutzt Pytorch, Torchvision sowie weitere Hilfsbibliotheken.\n",
    "\n",
    "Zudem werden die für die KI benötigte python Modul parsingnet eingebunden\n",
    "\n",
    "lane_colors sind die Farben in denen die Spur/-linien angezeigt wird.\n",
    "\n",
    "Der row Anchor gibt an, an welchen Orten die Spurpunkte sinnvoll sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import scipy.special\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from scipy.spatial.distance import cdist\n",
    "import time\n",
    "\n",
    "from model.model import parsingNet\n",
    "\n",
    "lane_colors = [(0,0,255),(0,255,0),(255,0,0)]\n",
    "\n",
    "tusimple_row_anchor = [ 64,  68,  72,  76,  80,  84,  88,  92,  96, 100, 104, 108, 112,\n",
    "\t\t\t116, 120, 124, 128, 132, 136, 140, 144, 148, 152, 156, 160, 164,\n",
    "\t\t\t168, 172, 176, 180, 184, 188, 192, 196, 200, 204, 208, 212, 216,\n",
    "\t\t\t220, 224, 228, 232, 236, 240, 244, 248, 252, 256, 260, 264, 268,\n",
    "\t\t\t272, 276, 280, 284]\n",
    "\n",
    "model_path = \"src/model/tusimple_18.pth\"\n",
    "useGPU = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haupt-Klasse\n",
    "Zur Besseren erklärung wird die Klasse LaneDetection weiter aufgespaltet.\n",
    "\n",
    "Die Klasse ModelConfig ist eine Hilfsklasse, die die generellen Konfigurationen des KI Modells bereitstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig():\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.imgWidth = 1280\n",
    "\t\tself.imgHeight = 720\n",
    "\t\tself.row_anchor = tusimple_row_anchor\n",
    "\t\tself.griding_num = 100\n",
    "\t\tself.cls_num_per_lane = 56"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Initialsierung import die Konfiguration, das vor trainierte KI Modell.\n",
    "Zudem wird die image Transformation zu der KI benötigten form initalisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneDetectiion():\n",
    "    def __init__(self, model_path, useGPU=False):\n",
    "\n",
    "        self.useGPU = useGPU\n",
    "\n",
    "        # Load model configuration\n",
    "        self.cfg = ModelConfig()\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = self.buildModel(model_path, self.cfg, useGPU)\n",
    "\n",
    "        # Initialize image transformation\n",
    "        self.img_transform = self.imageTransformation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In build Modell wird das trainierte KI Modell in die vorhandene Rechnerarchitektur eingebunden.\n",
    "Hirbei ist der größte Unterschied, ob die KI auf der Grafikkarte oder der CPU ausgeführt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def buildModel(self,model_path, cfg, useGPU):\n",
    "        # Load the model architecture\n",
    "        net = parsingNet(pretrained = False, backbone='18', cls_dim = (cfg.griding_num+1,cfg.cls_num_per_lane,4))\n",
    "\n",
    "\n",
    "        # Load the weights from the downloaded model\n",
    "        if useGPU:\n",
    "            net = net.cuda()\n",
    "            state_dict = torch.load(model_path, map_location='cuda')['model'] # CUDA\n",
    "        else:\n",
    "            state_dict = torch.load(model_path, map_location='cpu')['model'] # CPU\n",
    "\n",
    "        compatible_state_dict = {}\n",
    "        for k, v in state_dict.items():\n",
    "            if 'module.' in k:\n",
    "                compatible_state_dict[k[7:]] = v\n",
    "            else:\n",
    "                compatible_state_dict[k] = v\n",
    "\n",
    "        # Load the weights into the model\n",
    "        net.load_state_dict(compatible_state_dict, strict=False)\n",
    "        net.eval()\n",
    "\n",
    "        return net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die imageTransformation führt eine einfachte Transformation und Normalisierung der input Bilder durch, sodass die KI diese verarbeiten kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def imageTransformation(self):\n",
    "        img_transforms = transforms.Compose([\n",
    "\t\t\ttransforms.Resize((288, 800)),\n",
    "\t\t\ttransforms.ToTensor(),\n",
    "\t\t\ttransforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "\t\t])\n",
    "\n",
    "        return img_transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In detectLanes werden alle zur Spurerkennung benötigten Funktionen aufgerufen <br>\n",
    "1. Vorverarbeitung der Bilder (Transformation)\n",
    "2. KI auf Bild anwenden \n",
    "3. informationen aus KI extrahieren\n",
    "4. Linien und Punkte auf das Bild zeichnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def detectLanes(self, image, draw_points=True):\n",
    "\n",
    "        input_tensor = self.preprocess(image)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "\n",
    "        # Process output data\n",
    "        self.lanes_points, self.lanes_detected = self.process_output(output, self.cfg)\n",
    "\n",
    "\n",
    "        # Draw depth image\n",
    "        visualization_img = self.drawLanes(image, self.lanes_points, self.lanes_detected, self.cfg, draw_points)\n",
    "\n",
    "        return visualization_img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preprocess wird zunächst der Farbverlauf angepasst, dann eine PIL objekt erzeugt. Dieses Bild Objekt wird mittels der Image_transforma Funktion Normalisiert und transformiert. Zuletzt wird noch der Tensor für die KI erstellt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def preprocess(self, img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_pil = Image.fromarray(img)\n",
    "        input_img = self.img_transform(img_pil)\n",
    "        input_tensor = input_img[None, ...]\n",
    "\n",
    "        if self.useGPU:\n",
    "            if not torch.backends.mps.is_built():\n",
    "                input_tensor = input_tensor.cuda()\n",
    "\n",
    "        return input_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Process_Output Funktion werden die Informationen der KI extrahiert, sodass diese einfahc und performant dargestellt werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(self,output, cfg):\t\t\n",
    "        # Parse the output of the model\n",
    "        processed_output = output[0].data.cpu().numpy()\n",
    "        processed_output = processed_output[:, ::-1, :]\n",
    "        prob = scipy.special.softmax(processed_output[:-1, :, :], axis=0)\n",
    "        idx = np.arange(cfg.griding_num) + 1\n",
    "        idx = idx.reshape(-1, 1, 1)\n",
    "        loc = np.sum(prob * idx, axis=0)\n",
    "        processed_output = np.argmax(processed_output, axis=0)\n",
    "        loc[processed_output == cfg.griding_num] = 0\n",
    "        processed_output = loc\n",
    "\n",
    "\n",
    "        col_sample = np.linspace(0, 800 - 1, cfg.griding_num)\n",
    "        col_sample_w = col_sample[1] - col_sample[0]\n",
    "\n",
    "        lanes_points = []\n",
    "        lanes_detected = []\n",
    "\n",
    "        max_lanes = processed_output.shape[1]\n",
    "        for lane_num in range(max_lanes):\n",
    "            lane_points = []\n",
    "            # Check if there are any points detected in the lane\n",
    "            if np.sum(processed_output[:, lane_num] != 0) > 2:\n",
    "\n",
    "                lanes_detected.append(True)\n",
    "\n",
    "                # Process the first 26 points of each lane\n",
    "                for point_num in range(processed_output.shape[0]):\n",
    "                    if point_num > 26:\n",
    "                        pass\n",
    "                    else:\n",
    "                        if processed_output[point_num, lane_num] > 0:\n",
    "                            lane_point = [int(processed_output[point_num, lane_num] * col_sample_w * cfg.imgWidth / 800) - 1, int(cfg.imgHeight * (cfg.row_anchor[cfg.cls_num_per_lane-1-point_num]/288)) - 1 ]\n",
    "                            lane_points.append(lane_point)\n",
    "                            \n",
    "            else:\n",
    "                lanes_detected.append(False)\n",
    "\n",
    "            lanes_points.append(lane_points)\n",
    "        return np.array(lanes_points), np.array(lanes_detected)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die DrawLanes funktion zeichnet die ermittelten Linien und Punkte auf den jeweiligen Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def drawLanes(self,input_img, lanes_points, lanes_detected, cfg, draw_points=True):\n",
    "        # Write the detected line points in the image\n",
    "        visualization_img = cv2.resize(input_img, (cfg.imgWidth, cfg.imgHeight), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        # Draw a mask for the current lane\n",
    "        if(lanes_detected[1] and lanes_detected[2]):\n",
    "            lane_segment_img = visualization_img.copy()\n",
    "            \n",
    "            cv2.fillPoly(lane_segment_img, pts = [np.vstack((lanes_points[1],np.flipud(lanes_points[2])))], color =(255,191,0))\n",
    "            visualization_img = cv2.addWeighted(visualization_img, 0.7, lane_segment_img, 0.3, 0)\n",
    "\n",
    "        if(draw_points):\n",
    "            for lane_num,lane_points in enumerate(lanes_points):\n",
    "                if lane_num > 2:\n",
    "                    break\n",
    "                for lane_point in lane_points:\n",
    "                    cv2.circle(visualization_img, (lane_point[0],lane_point[1]), 3, lane_colors[lane_num], -1)\n",
    "\n",
    "        return visualization_img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die untensthende Prozedur führt die Lanedetecion auf einem der drei Videos durch und stellt die Ergebnisse in einem extra Fenster dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "cap = cv2.VideoCapture(\"img/Udacity/project_video.mp4\")\n",
    "\n",
    "# Initialize lane detection model\n",
    "lane_detector = LaneDetectiion(model_path, useGPU)\n",
    "\n",
    "cv2.namedWindow(\"Video\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "ret, frame = cap.read()\n",
    "while ret:\n",
    "\ttry:\n",
    "\t\t# Read frame from the video\n",
    "\t\tret, frame = cap.read()\n",
    "\t\t\n",
    "\texcept:\n",
    "\t\tcontinue\n",
    "\n",
    "\tif ret:\t\n",
    "\t\toutput_img = lane_detector.detectLanes(frame)\n",
    "\n",
    "\t\tcv2.imshow(\"Video\", output_img)\n",
    "\n",
    "\telse:\n",
    "\t\tbreak\n",
    "\n",
    "\t# Press key q to stop\n",
    "\tif cv2.waitKey(1) == ord('q'):\n",
    "\t\tbreak\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ressources:\n",
    "\n",
    "Model von: https://github.com/cfzd/Ultra-Fast-Lane-Detection <br>\n",
    "Direkt Dwonload: https://drive.google.com/file/d/1WCYyur5ZaWczH15ecmeDowrW30xcLrCn/view?usp=sharing\n",
    "\n",
    "\n",
    "Hilflibaries: https://github.com/ibaiGorordo/Ultrafast-Lane-Detection-Inference-Pytorch-/tree/main/ultrafastLaneDetector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Bildverarbeitung')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Oct 24 2022, 16:02:16) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0a3075694810a7bb993da77f9c64a653d81b0695ead550ad875383aa24f0556"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
